{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient1.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26970026, -0.29052939, -0.27212714, -0.27230177, -0.26312854]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            step = self.eta(iteraton)*self.calc_gradient(X[random_indexes], y[random_indexes]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26970026, -0.29052939, -0.27212714, -0.27230177, -0.26312854]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta(iteraton)*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26970026, -0.29052939, -0.27212714, -0.27230177, -0.26312854]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            self.g += calc_gradient(X[random_indexes], y[random_indexes]) ** 2\n",
    "            self.w -= self.eta(iteration) / ((self.g + self.eps)**0.5) * calc_gradient(X[random_indexes], y[random_indexes])\n",
    "        return self.w      \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26970026, -0.29052939, -0.27212714, -0.27230177, -0.26312854]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 gd_type='stochastic', \n",
    "                 tolerance=tolerance_default,\n",
    "                 max_iter=max_iter_default,\n",
    "                 w0=None,\n",
    "                 eta=1e-2,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w0 \n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w=np.zeros(np.size(X,1))\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                step=self.eta*self.calc_gradient(X,y) \n",
    "                w1=self.w.copy()\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                random_indexes = random.sample([range(len(y))], self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X[random_indexes], y[random_indexes])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return X.T.dot(X.dot(self.w)-y)/np.size(y)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sg=LinearReg(eta=0.01, batch_size=1, tolerance=0.001).fit(X, y)\n",
    "sg\n",
    "\n",
    "sns.lineplot(x=np.arange(sg.real_iter), y=sg.loss_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class MapGridTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, x1,y1,x2,y2,xc,yc):\n",
    "        self.x1 = x1\n",
    "        self.y1 = y1\n",
    "        self.x2 = x2\n",
    "        self.y2 = y2\n",
    "        self.xc = xc\n",
    "        self.yc = yc\n",
    "\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        x_grid = np.linspace(self.x1, self.x2, self.xc)\n",
    "        y_grid = np.linspace(self.y1, self.y2, self.yc)\n",
    "        last_x = len(x_grid)\n",
    "        last_y = len(y_grid)\n",
    "        \n",
    "        X = df.copy()\n",
    "\n",
    "        X['x_cell'] = X['pickup_longitude'].apply(lambda x: np.searchsorted(x_grid, x))\n",
    "        X['y_cell'] = X['pickup_latitude'].apply(lambda x: np.searchsorted(y_grid, x))\n",
    "        X['pickup_position'] = X.apply(lambda row: -1 if (row['x_cell']== 0 or row['y_cell']==0 or row['y_cell']==last_y or row['x_cell']==last_x) \\\n",
    "                                            else (row['y_cell'] * (self.xc-1) + row['x_cell']), axis=1)\n",
    "        \n",
    "        X['x_cell'] = X['dropoff_longitude'].apply(lambda x: np.searchsorted(x_grid, x))\n",
    "        X['y_cell'] = X['dropoff_latitude'].apply(lambda x: np.searchsorted(y_grid, x))\n",
    "        X['dropoff_position'] = X.apply(lambda row: -1 if (row['x_cell']== 0 or row['y_cell']==0 or row['y_cell']==last_y or row['x_cell']==last_x) \\\n",
    "                                            else (row['y_cell'] * (self.xc-1) + row['x_cell']), axis=1)       \n",
    "        \n",
    "        return X.drop(['x_cell', 'y_cell'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('../homework2/train.csv')\n",
    "df['log_trip_duration']=np.log1p(df['trip_duration'])\n",
    "\n",
    "df['pickup_datetime']=pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "df['hour']=df['pickup_datetime'].dt.hour\n",
    "df['weekday']=df['pickup_datetime'].dt.weekday\n",
    "df['day']=df['pickup_datetime'].dt.date\n",
    "df['daypor']=df['pickup_datetime'].dt.dayofyear\n",
    "df['month']=df['pickup_datetime'].dt.month\n",
    "\n",
    "\n",
    "hour = df['hour']\n",
    "df['free_road'] = ((hour > 3) & (hour < 7)).map(int)\n",
    "df['traffic_jam'] = ((hour > 9) & (hour < 15)).map(int)\n",
    "\n",
    "df['Kennedi_pickup'] = df.apply(lambda x: int(((40.633191)<=(x['pickup_latitude'])<=(40.660043))&((-73.793316)<= (x['pickup_longitude']) <=(-73.780070))), axis=1)\n",
    "df['Kennedi_dropoff'] = df.apply(lambda x: int(((40.633191)<=(x['dropoff_latitude'])<=(40.660043))&((-73.793316)<= (x['dropoff_longitude']) <=(-73.780070))), axis=1)\n",
    "\n",
    "df['Laguardia_pickup'] = df.apply(lambda x: int(((40.766937)<=(x['pickup_latitude'])<=(40.779799))&((-73.877414)<= (x['pickup_longitude']) <=(-73.8628986))), axis=1)\n",
    "df['Laguardia_dropoff'] = df.apply(lambda x: int(((40.766937)<=(x['dropoff_latitude'])<=(40.779799))&((-73.897414)<= (x['dropoff_longitude']) <=(-73.8622986))), axis=1)\n",
    "\n",
    "Mytransformer = MapGridTransformer(-74.014901, 40.701559, -73.930014, 40.801323, 5, 6)\n",
    "df = Mytransformer.fit_transform(df)\n",
    "\n",
    "popular_track = df[['dropoff_position', 'pickup_position']].value_counts().index[:30]\n",
    "df['popular_track'] = df.apply(lambda row: (str(row['dropoff_position']) + str(',') + str(row['pickup_position'])) \\\n",
    "                                                                        if (row['pickup_position'], row['dropoff_position']) in popular_track else str(-1), axis=1)\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.3, random_state=0)\n",
    "\n",
    "categorical = ['month', 'weekday', 'hour', 'free_road', 'traffic_jam', 'Kennedi_pickup', 'Kennedi_dropoff', 'Laguardia_pickup', 'Laguardia_dropoff', 'dropoff_position', 'pickup_position', 'popular_track' ]\n",
    "numeric_features = ['daypor']\n",
    "features=numeric_features+categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "X_test_scaled = scaler.fit_transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in np.logspace(-2, 0, 7):\n",
    "    print(i)\n",
    "    #full gd\n",
    "    gd_full=LinearReg(gd_type='full', eta=i).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_scaled)\n",
    "    y_pred = gd_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(eta=i, batch_size=1000).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_scaled)\n",
    "    y_pred = sg_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
