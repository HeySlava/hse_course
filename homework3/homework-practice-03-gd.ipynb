{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient1 = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient1.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26970026, -0.29052939, -0.27212714, -0.27230177, -0.26312854]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient1, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta*self.calc_gradient(X[self.batch_size], y[self.batch_size]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            k = np.random.randint(np.size(y))\n",
    "            self.g = self.g + calc_gradient(X[k], y[k]) ** 2\n",
    "            self.w = self.w - self.eta / ((self.g + self.eps)**0.5) * calc_gradient(X[k], y[k])\n",
    "        return self        \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='stochastic', \n",
    "                 tolerance=tolerance_default, max_iter=max_iter_default, w0=None, eta=1e-2,\n",
    "                batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w0 \n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w=np.zeros(np.size(X,1))\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                step=self.eta*self.calc_gradient(X,y) \n",
    "                w1=self.w.copy()\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "#                 print(self.read_iter)\n",
    "                sample_mask = random.sample([x for x in range(X.shape[0])], self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X[sample_mask], y[sample_mask])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return X.T.dot(X.dot(self.w)-y)/np.size(y)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sg=LinearReg(eta=0.01, batch_size=2).fit(X, y)\n",
    "sg\n",
    "\n",
    "sns.lineplot(x=np.arange(len(sg.loss_history)), y=sg.loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../homework2/train.csv')\n",
    "df['log_trip_duration']=np.log1p(df['trip_duration'])\n",
    "\n",
    "df['pickup_datetime']=pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "df['hour']=df['pickup_datetime'].dt.hour\n",
    "df['weekday']=df['pickup_datetime'].dt.weekday\n",
    "df['day']=df['pickup_datetime'].dt.date\n",
    "df['daypor']=df['pickup_datetime'].dt.dayofyear\n",
    "df['month']=df['pickup_datetime'].dt.month\n",
    "\n",
    "df['distance']=df.apply(lambda x: haversine((x['pickup_longitude'],x['pickup_latitude']),\n",
    "                                                      (x['dropoff_longitude'],x['dropoff_latitude'])), axis=1).values\n",
    "\n",
    "\n",
    "X_train, X_test = train_test_split(df, test_size=0.3, random_state=0)\n",
    "\n",
    "categorical=list(['month', 'weekday', 'hour', 'distance'])\n",
    "numeric_features=list(['daypor'])\n",
    "features=numeric_features+categorical\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train[features])\n",
    "X_test_scaled = scaler.fit_transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>25.329651</td>\n",
       "      <td>39.228257</td>\n",
       "      <td>25.354980</td>\n",
       "      <td>38.943635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>25.290207</td>\n",
       "      <td>39.165613</td>\n",
       "      <td>25.315144</td>\n",
       "      <td>38.880879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>25.065720</td>\n",
       "      <td>38.809084</td>\n",
       "      <td>25.086699</td>\n",
       "      <td>38.520991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>39.932453</td>\n",
       "      <td>62.420256</td>\n",
       "      <td>39.933882</td>\n",
       "      <td>61.910892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>42.453293</td>\n",
       "      <td>66.423826</td>\n",
       "      <td>42.454501</td>\n",
       "      <td>65.881816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>42.454107</td>\n",
       "      <td>66.425118</td>\n",
       "      <td>42.455314</td>\n",
       "      <td>65.883097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>42.454188</td>\n",
       "      <td>66.425248</td>\n",
       "      <td>42.455396</td>\n",
       "      <td>65.883225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            eta   test_mse    test_r2  train_mse   train_r2\n",
       "0  1.000000e-01  25.329651  39.228257  25.354980  38.943635\n",
       "1  1.000000e-02  25.290207  39.165613  25.315144  38.880879\n",
       "2  1.000000e-03  25.065720  38.809084  25.086699  38.520991\n",
       "3  1.000000e-04  39.932453  62.420256  39.933882  61.910892\n",
       "4  1.000000e-05  42.453293  66.423826  42.454501  65.881816\n",
       "5  1.000000e-06  42.454107  66.425118  42.455314  65.883097\n",
       "6  1.000000e-07  42.454188  66.425248  42.455396  65.883225"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in range(0, 7):\n",
    "    #full gd\n",
    "    gd_full=LinearReg(eta=0.1*(10**(-i))).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_scaled)\n",
    "    y_pred = gd_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(eta=0.1*(10**(-i)), batch_size=30).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_scaled)\n",
    "    y_pred = sg_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>35.510581</td>\n",
       "      <td>55.397491</td>\n",
       "      <td>35.480737</td>\n",
       "      <td>54.895513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>35.471623</td>\n",
       "      <td>55.335618</td>\n",
       "      <td>35.442264</td>\n",
       "      <td>54.834904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>35.166134</td>\n",
       "      <td>54.850444</td>\n",
       "      <td>35.141498</td>\n",
       "      <td>54.361084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>41.183953</td>\n",
       "      <td>64.407874</td>\n",
       "      <td>41.184808</td>\n",
       "      <td>63.881572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>42.453504</td>\n",
       "      <td>66.424161</td>\n",
       "      <td>42.454712</td>\n",
       "      <td>65.882148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>42.454128</td>\n",
       "      <td>66.425152</td>\n",
       "      <td>42.455335</td>\n",
       "      <td>65.883130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>42.454190</td>\n",
       "      <td>66.425251</td>\n",
       "      <td>42.455398</td>\n",
       "      <td>65.883229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            eta   test_mse    test_r2  train_mse   train_r2\n",
       "0  1.000000e-01  35.510581  55.397491  35.480737  54.895513\n",
       "1  1.000000e-02  35.471623  55.335618  35.442264  54.834904\n",
       "2  1.000000e-03  35.166134  54.850444  35.141498  54.361084\n",
       "3  1.000000e-04  41.183953  64.407874  41.184808  63.881572\n",
       "4  1.000000e-05  42.453504  66.424161  42.454712  65.882148\n",
       "5  1.000000e-06  42.454128  66.425152  42.455335  65.883130\n",
       "6  1.000000e-07  42.454190  66.425251  42.455398  65.883229"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "55\n",
      "105\n",
      "155\n",
      "205\n",
      "255\n",
      "305\n",
      "355\n",
      "405\n",
      "455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>iter_count</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.150420</td>\n",
       "      <td>8.768026</td>\n",
       "      <td>6.153454</td>\n",
       "      <td>8.694005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.119768</td>\n",
       "      <td>8.719344</td>\n",
       "      <td>6.122696</td>\n",
       "      <td>8.645551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.161055</td>\n",
       "      <td>8.784915</td>\n",
       "      <td>6.164193</td>\n",
       "      <td>8.710923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.217082</td>\n",
       "      <td>8.873897</td>\n",
       "      <td>6.220011</td>\n",
       "      <td>8.798859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.148853</td>\n",
       "      <td>8.765537</td>\n",
       "      <td>6.151649</td>\n",
       "      <td>8.691162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135889</td>\n",
       "      <td>8.744948</td>\n",
       "      <td>6.138895</td>\n",
       "      <td>8.671069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.122683</td>\n",
       "      <td>8.723974</td>\n",
       "      <td>6.125532</td>\n",
       "      <td>8.650018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.120752</td>\n",
       "      <td>8.720907</td>\n",
       "      <td>6.123632</td>\n",
       "      <td>8.647026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.134695</td>\n",
       "      <td>8.743052</td>\n",
       "      <td>6.137751</td>\n",
       "      <td>8.669268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.129581</td>\n",
       "      <td>8.734929</td>\n",
       "      <td>6.132525</td>\n",
       "      <td>8.661034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.142957</td>\n",
       "      <td>8.756173</td>\n",
       "      <td>6.145924</td>\n",
       "      <td>8.682143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.122180</td>\n",
       "      <td>8.723175</td>\n",
       "      <td>6.125120</td>\n",
       "      <td>8.649369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.136743</td>\n",
       "      <td>8.746304</td>\n",
       "      <td>6.139696</td>\n",
       "      <td>8.672332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.130741</td>\n",
       "      <td>8.736771</td>\n",
       "      <td>6.133646</td>\n",
       "      <td>8.662801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.128216</td>\n",
       "      <td>8.732761</td>\n",
       "      <td>6.131167</td>\n",
       "      <td>8.658896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135716</td>\n",
       "      <td>8.744672</td>\n",
       "      <td>6.138632</td>\n",
       "      <td>8.670655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.137355</td>\n",
       "      <td>8.747276</td>\n",
       "      <td>6.140282</td>\n",
       "      <td>8.673254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.127882</td>\n",
       "      <td>8.732231</td>\n",
       "      <td>6.130780</td>\n",
       "      <td>8.658285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.143452</td>\n",
       "      <td>8.756959</td>\n",
       "      <td>6.146473</td>\n",
       "      <td>8.683009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.142420</td>\n",
       "      <td>8.755320</td>\n",
       "      <td>6.145367</td>\n",
       "      <td>8.681266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.139907</td>\n",
       "      <td>8.751328</td>\n",
       "      <td>6.142894</td>\n",
       "      <td>8.677369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135934</td>\n",
       "      <td>8.745019</td>\n",
       "      <td>6.138838</td>\n",
       "      <td>8.670981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.131554</td>\n",
       "      <td>8.738062</td>\n",
       "      <td>6.134510</td>\n",
       "      <td>8.664161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.129287</td>\n",
       "      <td>8.734461</td>\n",
       "      <td>6.132196</td>\n",
       "      <td>8.660517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.132006</td>\n",
       "      <td>8.738780</td>\n",
       "      <td>6.134923</td>\n",
       "      <td>8.664812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.140625</td>\n",
       "      <td>8.752470</td>\n",
       "      <td>6.143627</td>\n",
       "      <td>8.678525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.134771</td>\n",
       "      <td>8.743171</td>\n",
       "      <td>6.137728</td>\n",
       "      <td>8.669231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.140850</td>\n",
       "      <td>8.752826</td>\n",
       "      <td>6.143803</td>\n",
       "      <td>8.678802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.129471</td>\n",
       "      <td>8.734754</td>\n",
       "      <td>6.132420</td>\n",
       "      <td>8.660870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.130975</td>\n",
       "      <td>8.737143</td>\n",
       "      <td>6.133892</td>\n",
       "      <td>8.663188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.134195</td>\n",
       "      <td>8.742257</td>\n",
       "      <td>6.137136</td>\n",
       "      <td>8.668299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135859</td>\n",
       "      <td>8.744900</td>\n",
       "      <td>6.138812</td>\n",
       "      <td>8.670939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.136081</td>\n",
       "      <td>8.745252</td>\n",
       "      <td>6.139033</td>\n",
       "      <td>8.671287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.126585</td>\n",
       "      <td>8.730170</td>\n",
       "      <td>6.129503</td>\n",
       "      <td>8.656275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.140856</td>\n",
       "      <td>8.752836</td>\n",
       "      <td>6.143818</td>\n",
       "      <td>8.678826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.136387</td>\n",
       "      <td>8.745739</td>\n",
       "      <td>6.139399</td>\n",
       "      <td>8.671864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135696</td>\n",
       "      <td>8.744641</td>\n",
       "      <td>6.138649</td>\n",
       "      <td>8.670682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.120283</td>\n",
       "      <td>8.720162</td>\n",
       "      <td>6.123206</td>\n",
       "      <td>8.646354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.137725</td>\n",
       "      <td>8.747864</td>\n",
       "      <td>6.140693</td>\n",
       "      <td>8.673902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.129130</td>\n",
       "      <td>8.734213</td>\n",
       "      <td>6.132037</td>\n",
       "      <td>8.660266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.134591</td>\n",
       "      <td>8.742887</td>\n",
       "      <td>6.137507</td>\n",
       "      <td>8.668884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.130187</td>\n",
       "      <td>8.735891</td>\n",
       "      <td>6.133109</td>\n",
       "      <td>8.661954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.136691</td>\n",
       "      <td>8.746221</td>\n",
       "      <td>6.139625</td>\n",
       "      <td>8.672219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.129063</td>\n",
       "      <td>8.734106</td>\n",
       "      <td>6.132010</td>\n",
       "      <td>8.660223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.135382</td>\n",
       "      <td>8.744143</td>\n",
       "      <td>6.138323</td>\n",
       "      <td>8.670168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.141168</td>\n",
       "      <td>8.753331</td>\n",
       "      <td>6.144079</td>\n",
       "      <td>8.679236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.136148</td>\n",
       "      <td>8.745358</td>\n",
       "      <td>6.139072</td>\n",
       "      <td>8.671349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.132791</td>\n",
       "      <td>8.740027</td>\n",
       "      <td>6.135719</td>\n",
       "      <td>8.666067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.134427</td>\n",
       "      <td>8.742626</td>\n",
       "      <td>6.137376</td>\n",
       "      <td>8.668677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.146410</td>\n",
       "      <td>8.761657</td>\n",
       "      <td>6.149425</td>\n",
       "      <td>8.687658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size iter_count  test_mse   test_r2  train_mse  train_r2\n",
       "0           5       1000  6.150420  8.768026   6.153454  8.694005\n",
       "1           5       1000  6.119768  8.719344   6.122696  8.645551\n",
       "2           5       1000  6.161055  8.784915   6.164193  8.710923\n",
       "3           5       1000  6.217082  8.873897   6.220011  8.798859\n",
       "4           5       1000  6.148853  8.765537   6.151649  8.691162\n",
       "5          55       1000  6.135889  8.744948   6.138895  8.671069\n",
       "6          55       1000  6.122683  8.723974   6.125532  8.650018\n",
       "7          55       1000  6.120752  8.720907   6.123632  8.647026\n",
       "8          55       1000  6.134695  8.743052   6.137751  8.669268\n",
       "9          55       1000  6.129581  8.734929   6.132525  8.661034\n",
       "10        105       1000  6.142957  8.756173   6.145924  8.682143\n",
       "11        105       1000  6.122180  8.723175   6.125120  8.649369\n",
       "12        105       1000  6.136743  8.746304   6.139696  8.672332\n",
       "13        105       1000  6.130741  8.736771   6.133646  8.662801\n",
       "14        105       1000  6.128216  8.732761   6.131167  8.658896\n",
       "15        155       1000  6.135716  8.744672   6.138632  8.670655\n",
       "16        155       1000  6.137355  8.747276   6.140282  8.673254\n",
       "17        155       1000  6.127882  8.732231   6.130780  8.658285\n",
       "18        155       1000  6.143452  8.756959   6.146473  8.683009\n",
       "19        155       1000  6.142420  8.755320   6.145367  8.681266\n",
       "20        205       1000  6.139907  8.751328   6.142894  8.677369\n",
       "21        205       1000  6.135934  8.745019   6.138838  8.670981\n",
       "22        205       1000  6.131554  8.738062   6.134510  8.664161\n",
       "23        205       1000  6.129287  8.734461   6.132196  8.660517\n",
       "24        205       1000  6.132006  8.738780   6.134923  8.664812\n",
       "25        255       1000  6.140625  8.752470   6.143627  8.678525\n",
       "26        255       1000  6.134771  8.743171   6.137728  8.669231\n",
       "27        255       1000  6.140850  8.752826   6.143803  8.678802\n",
       "28        255       1000  6.129471  8.734754   6.132420  8.660870\n",
       "29        255       1000  6.130975  8.737143   6.133892  8.663188\n",
       "30        305       1000  6.134195  8.742257   6.137136  8.668299\n",
       "31        305       1000  6.135859  8.744900   6.138812  8.670939\n",
       "32        305       1000  6.136081  8.745252   6.139033  8.671287\n",
       "33        305       1000  6.126585  8.730170   6.129503  8.656275\n",
       "34        305       1000  6.140856  8.752836   6.143818  8.678826\n",
       "35        355       1000  6.136387  8.745739   6.139399  8.671864\n",
       "36        355       1000  6.135696  8.744641   6.138649  8.670682\n",
       "37        355       1000  6.120283  8.720162   6.123206  8.646354\n",
       "38        355       1000  6.137725  8.747864   6.140693  8.673902\n",
       "39        355       1000  6.129130  8.734213   6.132037  8.660266\n",
       "40        405       1000  6.134591  8.742887   6.137507  8.668884\n",
       "41        405       1000  6.130187  8.735891   6.133109  8.661954\n",
       "42        405       1000  6.136691  8.746221   6.139625  8.672219\n",
       "43        405       1000  6.129063  8.734106   6.132010  8.660223\n",
       "44        405       1000  6.135382  8.744143   6.138323  8.670168\n",
       "45        455       1000  6.141168  8.753331   6.144079  8.679236\n",
       "46        455       1000  6.136148  8.745358   6.139072  8.671349\n",
       "47        455       1000  6.132791  8.740027   6.135719  8.666067\n",
       "48        455       1000  6.134427  8.742626   6.137376  8.668677\n",
       "49        455       1000  6.146410  8.761657   6.149425  8.687658"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
