{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.23999325, -0.24724991, -0.24151042, -0.26330689, -0.2869889 ]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            step = self.eta(iteraton)*self.calc_gradient(X[random_indexes], y[random_indexes]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.23999325, -0.24724991, -0.24151042, -0.26330689, -0.2869889 ]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta(iteraton)*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.23999325, -0.24724991, -0.24151042, -0.26330689, -0.2869889 ]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            self.g += calc_gradient(X[random_indexes], y[random_indexes]) ** 2\n",
    "            self.w -= self.eta(iteration) / ((self.g + self.eps)**0.5) * calc_gradient(X[random_indexes], y[random_indexes])\n",
    "        return self.w      \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.23999325, -0.24724991, -0.24151042, -0.26330689, -0.2869889 ]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 gd_type='stochastic', \n",
    "                 tolerance=tolerance_default,\n",
    "                 max_iter=max_iter_default,\n",
    "                 w0=None,\n",
    "                 eta=1e-2,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w0 \n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w=np.zeros(np.size(X,1))\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                step=self.eta*self.calc_gradient(X,y) \n",
    "                w1=self.w.copy()\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X[random_indexes], y[random_indexes])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return X.T.dot(X.dot(self.w)-y)/np.size(y)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sg=LinearReg(eta=0.01, batch_size=10, tolerance=0.001).fit(X, y)\n",
    "sg\n",
    "\n",
    "sns.lineplot(x=np.arange(sg.real_iter), y=sg.loss_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex1data2.txt', header=None, names=['X', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['X'].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gd = LinearReg(batch_size=10).fit(X, y)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "ax = sns.lineplot(x=np.arange(gd.real_iter), y=gd.loss_history)\n",
    "plt.xlim([0, 50]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.021544346900318832\n",
      "0.046415888336127774\n",
      "0.1\n",
      "0.21544346900318834\n",
      "0.46415888336127775\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/.local/lib/python3.8/site-packages/numpy/core/_methods.py:178: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:603: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:258: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "/home/vyacheslav/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py:603: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097724</td>\n",
       "      <td>0.561980</td>\n",
       "      <td>-0.116169</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>-0.121416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095160</td>\n",
       "      <td>0.555604</td>\n",
       "      <td>-0.126197</td>\n",
       "      <td>0.541751</td>\n",
       "      <td>-0.133730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089864</td>\n",
       "      <td>0.554609</td>\n",
       "      <td>-0.127762</td>\n",
       "      <td>0.540157</td>\n",
       "      <td>-0.136278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.079433</td>\n",
       "      <td>0.554550</td>\n",
       "      <td>-0.127855</td>\n",
       "      <td>0.539829</td>\n",
       "      <td>-0.136802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060891</td>\n",
       "      <td>0.554612</td>\n",
       "      <td>-0.127756</td>\n",
       "      <td>0.539757</td>\n",
       "      <td>-0.136918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034343</td>\n",
       "      <td>0.554653</td>\n",
       "      <td>-0.127693</td>\n",
       "      <td>0.539747</td>\n",
       "      <td>-0.136934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.554659</td>\n",
       "      <td>-0.127682</td>\n",
       "      <td>0.539739</td>\n",
       "      <td>-0.136946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        eta  test_mse   test_r2  train_mse  train_r2\n",
       "0  0.097724  0.561980 -0.116169   0.549452 -0.121416\n",
       "1  0.095160  0.555604 -0.126197   0.541751 -0.133730\n",
       "2  0.089864  0.554609 -0.127762   0.540157 -0.136278\n",
       "3  0.079433  0.554550 -0.127855   0.539829 -0.136802\n",
       "4  0.060891  0.554612 -0.127756   0.539757 -0.136918\n",
       "5  0.034343  0.554653 -0.127693   0.539747 -0.136934\n",
       "6  0.010000  0.554659 -0.127682   0.539739 -0.136946"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in np.logspace(-2, 0, 7):\n",
    "    print(i)\n",
    "    #full gd\n",
    "    gd_full=LinearReg(gd_type='full', eta=i).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_scaled)\n",
    "    y_pred = gd_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(eta=i, batch_size=10).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_scaled)\n",
    "    y_pred = sg_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eta</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097724</td>\n",
       "      <td>5.629177e-01</td>\n",
       "      <td>-1.146941e-01</td>\n",
       "      <td>5.470020e-01</td>\n",
       "      <td>-1.253326e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095160</td>\n",
       "      <td>5.662446e-01</td>\n",
       "      <td>-1.094618e-01</td>\n",
       "      <td>5.489077e-01</td>\n",
       "      <td>-1.222854e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089864</td>\n",
       "      <td>6.179563e-01</td>\n",
       "      <td>-2.813428e-02</td>\n",
       "      <td>6.108605e-01</td>\n",
       "      <td>-2.322165e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.079433</td>\n",
       "      <td>6.060166e-01</td>\n",
       "      <td>-4.691195e-02</td>\n",
       "      <td>5.894397e-01</td>\n",
       "      <td>-5.747387e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060891</td>\n",
       "      <td>5.514247e+52</td>\n",
       "      <td>8.672308e+52</td>\n",
       "      <td>5.519209e+52</td>\n",
       "      <td>8.825328e+52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034343</td>\n",
       "      <td>9.689739e+139</td>\n",
       "      <td>1.523914e+140</td>\n",
       "      <td>9.661545e+139</td>\n",
       "      <td>1.544901e+140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        eta       test_mse        test_r2      train_mse       train_r2\n",
       "0  0.097724   5.629177e-01  -1.146941e-01   5.470020e-01  -1.253326e-01\n",
       "1  0.095160   5.662446e-01  -1.094618e-01   5.489077e-01  -1.222854e-01\n",
       "2  0.089864   6.179563e-01  -2.813428e-02   6.108605e-01  -2.322165e-02\n",
       "3  0.079433   6.060166e-01  -4.691195e-02   5.894397e-01  -5.747387e-02\n",
       "4  0.060891   5.514247e+52   8.672308e+52   5.519209e+52   8.825328e+52\n",
       "5  0.034343  9.689739e+139  1.523914e+140  9.661545e+139  1.544901e+140\n",
       "6  0.010000            inf            inf            inf            inf"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "55\n",
      "105\n",
      "155\n",
      "205\n",
      "255\n",
      "305\n",
      "355\n",
      "405\n",
      "455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>iter_count</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.233628</td>\n",
       "      <td>8.803686</td>\n",
       "      <td>6.280063</td>\n",
       "      <td>9.041949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.243574</td>\n",
       "      <td>8.819328</td>\n",
       "      <td>6.290489</td>\n",
       "      <td>9.058620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.182281</td>\n",
       "      <td>8.722931</td>\n",
       "      <td>6.227421</td>\n",
       "      <td>8.957772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.213900</td>\n",
       "      <td>8.772660</td>\n",
       "      <td>6.258256</td>\n",
       "      <td>9.007079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.215190</td>\n",
       "      <td>8.774687</td>\n",
       "      <td>6.258334</td>\n",
       "      <td>9.007204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.154519</td>\n",
       "      <td>8.679270</td>\n",
       "      <td>6.198566</td>\n",
       "      <td>8.911633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.168464</td>\n",
       "      <td>8.701202</td>\n",
       "      <td>6.213537</td>\n",
       "      <td>8.935571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.182762</td>\n",
       "      <td>8.723688</td>\n",
       "      <td>6.227631</td>\n",
       "      <td>8.958109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.172679</td>\n",
       "      <td>8.707830</td>\n",
       "      <td>6.217454</td>\n",
       "      <td>8.941835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.165887</td>\n",
       "      <td>8.697149</td>\n",
       "      <td>6.211033</td>\n",
       "      <td>8.931568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.156573</td>\n",
       "      <td>8.682500</td>\n",
       "      <td>6.201993</td>\n",
       "      <td>8.917113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.155323</td>\n",
       "      <td>8.680535</td>\n",
       "      <td>6.199859</td>\n",
       "      <td>8.913700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.164749</td>\n",
       "      <td>8.695359</td>\n",
       "      <td>6.209674</td>\n",
       "      <td>8.929394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.158649</td>\n",
       "      <td>8.685765</td>\n",
       "      <td>6.203262</td>\n",
       "      <td>8.919142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>105</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.169788</td>\n",
       "      <td>8.703283</td>\n",
       "      <td>6.215218</td>\n",
       "      <td>8.938261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.171692</td>\n",
       "      <td>8.706278</td>\n",
       "      <td>6.216851</td>\n",
       "      <td>8.940870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.163346</td>\n",
       "      <td>8.693153</td>\n",
       "      <td>6.208615</td>\n",
       "      <td>8.927702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.157348</td>\n",
       "      <td>8.683720</td>\n",
       "      <td>6.202248</td>\n",
       "      <td>8.917521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.166232</td>\n",
       "      <td>8.697691</td>\n",
       "      <td>6.211230</td>\n",
       "      <td>8.931883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>155</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.173245</td>\n",
       "      <td>8.708721</td>\n",
       "      <td>6.218058</td>\n",
       "      <td>8.942802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.155395</td>\n",
       "      <td>8.680648</td>\n",
       "      <td>6.200524</td>\n",
       "      <td>8.914764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.177883</td>\n",
       "      <td>8.716014</td>\n",
       "      <td>6.222689</td>\n",
       "      <td>8.950207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.170296</td>\n",
       "      <td>8.704083</td>\n",
       "      <td>6.215099</td>\n",
       "      <td>8.938070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.157889</td>\n",
       "      <td>8.684570</td>\n",
       "      <td>6.202842</td>\n",
       "      <td>8.918471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>205</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.165098</td>\n",
       "      <td>8.695908</td>\n",
       "      <td>6.209726</td>\n",
       "      <td>8.929478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.166089</td>\n",
       "      <td>8.697466</td>\n",
       "      <td>6.211368</td>\n",
       "      <td>8.932104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.161936</td>\n",
       "      <td>8.690935</td>\n",
       "      <td>6.206621</td>\n",
       "      <td>8.924513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.168514</td>\n",
       "      <td>8.701280</td>\n",
       "      <td>6.213897</td>\n",
       "      <td>8.936147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.159898</td>\n",
       "      <td>8.687730</td>\n",
       "      <td>6.204776</td>\n",
       "      <td>8.921563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>255</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.160947</td>\n",
       "      <td>8.689379</td>\n",
       "      <td>6.206094</td>\n",
       "      <td>8.923671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.162616</td>\n",
       "      <td>8.692005</td>\n",
       "      <td>6.207264</td>\n",
       "      <td>8.925542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.162737</td>\n",
       "      <td>8.692195</td>\n",
       "      <td>6.207628</td>\n",
       "      <td>8.926123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.166181</td>\n",
       "      <td>8.697611</td>\n",
       "      <td>6.210896</td>\n",
       "      <td>8.931350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.161967</td>\n",
       "      <td>8.690984</td>\n",
       "      <td>6.207228</td>\n",
       "      <td>8.925484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>305</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.160693</td>\n",
       "      <td>8.688979</td>\n",
       "      <td>6.205279</td>\n",
       "      <td>8.922368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.158042</td>\n",
       "      <td>8.684811</td>\n",
       "      <td>6.202670</td>\n",
       "      <td>8.918195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.159637</td>\n",
       "      <td>8.687319</td>\n",
       "      <td>6.204507</td>\n",
       "      <td>8.921133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.168971</td>\n",
       "      <td>8.701999</td>\n",
       "      <td>6.213574</td>\n",
       "      <td>8.935631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.163140</td>\n",
       "      <td>8.692828</td>\n",
       "      <td>6.208164</td>\n",
       "      <td>8.926980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>355</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.159861</td>\n",
       "      <td>8.687671</td>\n",
       "      <td>6.204717</td>\n",
       "      <td>8.921469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.159671</td>\n",
       "      <td>8.687373</td>\n",
       "      <td>6.204607</td>\n",
       "      <td>8.921293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.170617</td>\n",
       "      <td>8.704588</td>\n",
       "      <td>6.215790</td>\n",
       "      <td>8.939175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.158800</td>\n",
       "      <td>8.686002</td>\n",
       "      <td>6.203720</td>\n",
       "      <td>8.919875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.168116</td>\n",
       "      <td>8.700655</td>\n",
       "      <td>6.212863</td>\n",
       "      <td>8.934495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>405</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.165724</td>\n",
       "      <td>8.696892</td>\n",
       "      <td>6.210734</td>\n",
       "      <td>8.931091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.164726</td>\n",
       "      <td>8.695323</td>\n",
       "      <td>6.209629</td>\n",
       "      <td>8.929323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.167441</td>\n",
       "      <td>8.699593</td>\n",
       "      <td>6.212450</td>\n",
       "      <td>8.933834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.154300</td>\n",
       "      <td>8.678926</td>\n",
       "      <td>6.199056</td>\n",
       "      <td>8.912417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.166168</td>\n",
       "      <td>8.697591</td>\n",
       "      <td>6.211001</td>\n",
       "      <td>8.931518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>455</td>\n",
       "      <td>1000</td>\n",
       "      <td>6.151793</td>\n",
       "      <td>8.674983</td>\n",
       "      <td>6.196461</td>\n",
       "      <td>8.908268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size iter_count  test_mse   test_r2  train_mse  train_r2\n",
       "0           5       1000  6.233628  8.803686   6.280063  9.041949\n",
       "1           5       1000  6.243574  8.819328   6.290489  9.058620\n",
       "2           5       1000  6.182281  8.722931   6.227421  8.957772\n",
       "3           5       1000  6.213900  8.772660   6.258256  9.007079\n",
       "4           5       1000  6.215190  8.774687   6.258334  9.007204\n",
       "5          55       1000  6.154519  8.679270   6.198566  8.911633\n",
       "6          55       1000  6.168464  8.701202   6.213537  8.935571\n",
       "7          55       1000  6.182762  8.723688   6.227631  8.958109\n",
       "8          55       1000  6.172679  8.707830   6.217454  8.941835\n",
       "9          55       1000  6.165887  8.697149   6.211033  8.931568\n",
       "10        105       1000  6.156573  8.682500   6.201993  8.917113\n",
       "11        105       1000  6.155323  8.680535   6.199859  8.913700\n",
       "12        105       1000  6.164749  8.695359   6.209674  8.929394\n",
       "13        105       1000  6.158649  8.685765   6.203262  8.919142\n",
       "14        105       1000  6.169788  8.703283   6.215218  8.938261\n",
       "15        155       1000  6.171692  8.706278   6.216851  8.940870\n",
       "16        155       1000  6.163346  8.693153   6.208615  8.927702\n",
       "17        155       1000  6.157348  8.683720   6.202248  8.917521\n",
       "18        155       1000  6.166232  8.697691   6.211230  8.931883\n",
       "19        155       1000  6.173245  8.708721   6.218058  8.942802\n",
       "20        205       1000  6.155395  8.680648   6.200524  8.914764\n",
       "21        205       1000  6.177883  8.716014   6.222689  8.950207\n",
       "22        205       1000  6.170296  8.704083   6.215099  8.938070\n",
       "23        205       1000  6.157889  8.684570   6.202842  8.918471\n",
       "24        205       1000  6.165098  8.695908   6.209726  8.929478\n",
       "25        255       1000  6.166089  8.697466   6.211368  8.932104\n",
       "26        255       1000  6.161936  8.690935   6.206621  8.924513\n",
       "27        255       1000  6.168514  8.701280   6.213897  8.936147\n",
       "28        255       1000  6.159898  8.687730   6.204776  8.921563\n",
       "29        255       1000  6.160947  8.689379   6.206094  8.923671\n",
       "30        305       1000  6.162616  8.692005   6.207264  8.925542\n",
       "31        305       1000  6.162737  8.692195   6.207628  8.926123\n",
       "32        305       1000  6.166181  8.697611   6.210896  8.931350\n",
       "33        305       1000  6.161967  8.690984   6.207228  8.925484\n",
       "34        305       1000  6.160693  8.688979   6.205279  8.922368\n",
       "35        355       1000  6.158042  8.684811   6.202670  8.918195\n",
       "36        355       1000  6.159637  8.687319   6.204507  8.921133\n",
       "37        355       1000  6.168971  8.701999   6.213574  8.935631\n",
       "38        355       1000  6.163140  8.692828   6.208164  8.926980\n",
       "39        355       1000  6.159861  8.687671   6.204717  8.921469\n",
       "40        405       1000  6.159671  8.687373   6.204607  8.921293\n",
       "41        405       1000  6.170617  8.704588   6.215790  8.939175\n",
       "42        405       1000  6.158800  8.686002   6.203720  8.919875\n",
       "43        405       1000  6.168116  8.700655   6.212863  8.934495\n",
       "44        405       1000  6.165724  8.696892   6.210734  8.931091\n",
       "45        455       1000  6.164726  8.695323   6.209629  8.929323\n",
       "46        455       1000  6.167441  8.699593   6.212450  8.933834\n",
       "47        455       1000  6.154300  8.678926   6.199056  8.912417\n",
       "48        455       1000  6.166168  8.697591   6.211001  8.931518\n",
       "49        455       1000  6.151793  8.674983   6.196461  8.908268"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
