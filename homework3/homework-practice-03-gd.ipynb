{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            step = self.eta(iteraton)*self.calc_gradient(X[random_indexes], y[random_indexes]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta(iteraton)*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            self.g += calc_gradient(X[random_indexes], y[random_indexes]) ** 2\n",
    "            self.w -= self.eta(iteration) / ((self.g + self.eps)**0.5) * calc_gradient(X[random_indexes], y[random_indexes])\n",
    "        return self.w      \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 gd_type='stochastic', \n",
    "                 tolerance=tolerance_default,\n",
    "                 max_iter=max_iter_default,\n",
    "                 weight=None,\n",
    "                 eta=1e-2,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = weight\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w = np.zeros(shape=X.shape[1])\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                step = self.eta * self.calc_gradient(X,y)\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(step) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                random_indexes = np.random.randint(X.shape[0], size=self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X.iloc[random_indexes], y[random_indexes])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "#         print(f'size of weight {self.w.shape}')\n",
    "#         print(f'size of X = {X.shape}', f'size of y_pred = {(X@self.w - y).shape}', f'size of y {y.shape}')\n",
    "\n",
    "#         return (X.T @ (X@self.w - y)) / np.size(y)\n",
    "        return (X.T.dot(X.dot(self.w) - y))/ X.shape[0]\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "df = pd.read_csv('autos.csv')\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "y = df['log_price']\n",
    "df = df.drop(columns = ['dateCreated', 'lastSeen','postalCode','price', 'log_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.4, random_state=42)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = df.select_dtypes([np.number]).columns\n",
    "categorical = list(df.dtypes[df.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trainsform = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('SS', StandardScaler(), numeric)\n",
    "])\n",
    "\n",
    "X_train_tf = column_trainsform.fit_transform(X_train)\n",
    "X_test_tf = column_trainsform.fit_transform(X_test)\n",
    "X_valid_tf = column_trainsform.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.873632</td>\n",
       "      <td>0.658371</td>\n",
       "      <td>-0.075759</td>\n",
       "      <td>0.227382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  301  302  303  304  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   305  306       307       308       309       310  \n",
       "0  1.0  0.0 -0.873632  0.658371 -0.075759  0.227382  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = pd.DataFrame.sparse.from_spmatrix(X_train_tf)\n",
    "tt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.577122</td>\n",
       "      <td>0.658371</td>\n",
       "      <td>-0.075759</td>\n",
       "      <td>-0.934572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  301  302  303  \\\n",
       "91780  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       304  305  306       307       308       309       310  \n",
       "91780  0.0  0.0  1.0  5.577122  0.658371 -0.075759 -0.934572  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(X_train_tf.shape[0],size=10).tolist()\n",
    "tt.iloc[idx].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<148723x311 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1487230 stored elements in Compressed Sparse Row format>,\n",
       " (148723,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gd = LinearReg(tolerance=0.001, batch_size=1).fit(tt, y_train.reset_index(drop=True))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "ax = sns.lineplot(x=np.arange(gd.real_iter), y=gd.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 48966,  38544,  25922, 117612,  78132,  79239, 131422,  37216,\n",
       "        42486,  33511])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(X_train_tf.shape[0],size=10)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x311 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in np.logspace(-2, 0, 7):\n",
    "    print(i)\n",
    "    #full gd\n",
    "    gd_full=LinearReg(gd_type='full', eta=i).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_scaled)\n",
    "    y_pred = gd_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(eta=i, batch_size=10).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_scaled)\n",
    "    y_pred = sg_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
