{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26640824, -0.28437732, -0.29128247, -0.27773697, -0.29172162]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            step = self.eta(iteraton)*self.calc_gradient(X[random_indexes], y[random_indexes]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26640824, -0.28437732, -0.29128247, -0.27773697, -0.29172162]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta(iteraton)*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26640824, -0.28437732, -0.29128247, -0.27773697, -0.29172162]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            self.g += calc_gradient(X[random_indexes], y[random_indexes]) ** 2\n",
    "            self.w -= self.eta(iteration) / ((self.g + self.eps)**0.5) * calc_gradient(X[random_indexes], y[random_indexes])\n",
    "        return self.w      \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.26640824, -0.28437732, -0.29128247, -0.27773697, -0.29172162]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 gd_type='stochastic', \n",
    "                 tolerance=tolerance_default,\n",
    "                 max_iter=max_iter_default,\n",
    "                 weight=None,\n",
    "                 eta=1e-2,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = weight\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "#         X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w = np.zeros(shape=X.shape[1])\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                step = self.eta * self.calc_gradient(X,y)\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(step) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                random_indexes = np.random.randint(X.shape[0], size=self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X[random_indexes], y[random_indexes])  \n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(step) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "#         print(f'size of weight {self.w.shape}')\n",
    "#         print(f'size of X = {X.shape}', f'size of y_pred = {(X@self.w - y).shape}', f'size of y {y.shape}')\n",
    "\n",
    "#         return (X.T @ (X@self.w - y)) / np.size(y)\n",
    "        return (X.T.dot(X.dot(self.w) - y))/ X.shape[0]\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "df = pd.read_csv('autos.csv')\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "y = df['log_price']\n",
    "df = df.drop(columns = ['dateCreated', 'lastSeen','postalCode','price', 'log_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.4, random_state=42)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = df.select_dtypes([np.number]).columns\n",
    "categorical = list(df.dtypes[df.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trainsform = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('SS', StandardScaler(), numeric)\n",
    "])\n",
    "\n",
    "X_train_tf = column_trainsform.fit_transform(X_train)\n",
    "X_test_tf = column_trainsform.fit_transform(X_test)\n",
    "X_valid_tf = column_trainsform.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.873632</td>\n",
       "      <td>0.658371</td>\n",
       "      <td>-0.075759</td>\n",
       "      <td>0.227382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  301  302  303  304  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   305  306       307       308       309       310  \n",
       "0  1.0  0.0 -0.873632  0.658371 -0.075759  0.227382  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = pd.DataFrame.sparse.from_spmatrix(X_train_tf)\n",
    "tt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34636</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.391032</td>\n",
       "      <td>-1.855012</td>\n",
       "      <td>1.643193</td>\n",
       "      <td>-0.934572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  301  302  303  \\\n",
       "34636  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  0.0  0.0   \n",
       "\n",
       "       304  305  306       307       308       309       310  \n",
       "34636  0.0  0.0  1.0 -0.391032 -1.855012  1.643193 -0.934572  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(X_train_tf.shape[0],size=10).tolist()\n",
    "tt.iloc[idx].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<148723x311 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1487230 stored elements in Compressed Sparse Row format>,\n",
       " (148723,))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gd = LinearReg(tolerance=0.01, batch_size=1).fit(X_valid_tf, y_valid.reset_index(drop=True))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "ax = sns.lineplot(x=np.arange(gd.real_iter), y=gd.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27290, 96110,  6117, 34848,  6633, 30175, 30808,  1372, 95597,\n",
       "       48032])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(X_train_tf.shape[0],size=10)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x311 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 100 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.2166061 , 8.00336306, 8.64839688, ..., 7.09090982, 9.25922577,\n",
       "       8.78630388])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-7db7aa281a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-03827046a470>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not trained yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalc_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# dense row or column vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in np.logspace(-2, 0, 7):\n",
    "    print(i)\n",
    "    #full gd\n",
    "    gd_full=LinearReg(gd_type='full', tolerance=0.01).fit(X_train_tf, y_train.reset_index(drop=True))\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_tf)\n",
    "    y_pred = gd_full.predict(X_test_tf)\n",
    "    \n",
    "    test_mse = mse(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    train_mse = mse(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(tolerance=0.01, batch_size=1).fit(X_train_tf, y_train.reset_index(drop=True))\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_tf)\n",
    "    y_pred = sg_full.predict(X_test_tf)\n",
    "    \n",
    "    test_mse = mse(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    train_mse = mse(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
