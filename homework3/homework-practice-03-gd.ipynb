{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta(iteration) * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            step = self.eta(iteraton)*self.calc_gradient(X[random_indexes], y[random_indexes]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta(iteraton)*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self.w\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            random_indexes = random.sample([i for i in range(len(y))], self.batch_size)\n",
    "            self.g += calc_gradient(X[random_indexes], y[random_indexes]) ** 2\n",
    "            self.w -= self.eta(iteration) / ((self.g + self.eps)**0.5) * calc_gradient(X[random_indexes], y[random_indexes])\n",
    "        return self.w      \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "diff = descent.step(X, y, 0)\n",
    "\n",
    "assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.25765717, -0.26289494, -0.28457763, -0.24823874, -0.26031548]),\n",
       " array([0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(shape=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self,\n",
    "                 gd_type='stochastic', \n",
    "                 tolerance=tolerance_default,\n",
    "                 max_iter=max_iter_default,\n",
    "                 weight=None,\n",
    "                 eta=1e-2,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = weight\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "        self.real_iter = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w = np.zeros(shape=X.shape[1])\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                step = self.eta * self.calc_gradient(X,y)\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(step) < self.tolerance: \n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                self.real_iter += 1\n",
    "                random_indexes = np.random.randint(X.shape[0], size=self.batch_size)\n",
    "                step=self.eta*self.calc_gradient(X.iloc[random_indexes], y[random_indexes])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "#         print(f'size of weight {self.w.shape}')\n",
    "#         print(f'size of X = {X.shape}', f'size of y_pred = {(X@self.w - y).shape}', f'size of y {y.shape}')\n",
    "\n",
    "#         return (X.T @ (X@self.w - y)) / np.size(y)\n",
    "        return (X.T.dot(X.dot(self.w) - y))/ X.shape[0]\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:\n",
    "df = pd.read_csv('autos.csv')\n",
    "df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "y = df['log_price']\n",
    "df = df.drop(columns = ['dateCreated', 'lastSeen','postalCode','price', 'log_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.4, random_state=42)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = df.select_dtypes([np.number]).columns\n",
    "categorical = list(df.dtypes[df.dtypes == \"object\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trainsform = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), categorical),\n",
    "    ('SS', StandardScaler(), numeric)\n",
    "])\n",
    "\n",
    "X_train_tf = column_trainsform.fit_transform(X_train)\n",
    "X_test_tf = column_trainsform.fit_transform(X_test)\n",
    "X_valid_tf = column_trainsform.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.873632</td>\n",
       "      <td>0.658371</td>\n",
       "      <td>-0.075759</td>\n",
       "      <td>0.227382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  301  302  303  304  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   305  306       307       308       309       310  \n",
       "0  1.0  0.0 -0.873632  0.658371 -0.075759  0.227382  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = pd.DataFrame.sparse.from_spmatrix(X_train_tf)\n",
    "tt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.577122</td>\n",
       "      <td>0.658371</td>\n",
       "      <td>-0.075759</td>\n",
       "      <td>-0.934572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  301  302  303  \\\n",
       "91780  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       304  305  306       307       308       309       310  \n",
       "91780  0.0  0.0  1.0  5.577122  0.658371 -0.075759 -0.934572  \n",
       "\n",
       "[1 rows x 311 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(X_train_tf.shape[0],size=10).tolist()\n",
    "tt.iloc[idx].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<148723x311 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1487230 stored elements in Compressed Sparse Row format>,\n",
       " (148723,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gd = LinearReg(tolerance=0.001, batch_size=1).fit(tt, y_train.reset_index(drop=True))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 7))\n",
    "ax = sns.lineplot(x=np.arange(gd.real_iter), y=gd.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = ['eta', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "full_df = pd.DataFrame(columns=columns)\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for i in np.logspace(-2, 0, 7):\n",
    "    print(i)\n",
    "    #full gd\n",
    "    gd_full=LinearReg(gd_type='full', eta=i).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "    y_train_pred = gd_full.predict(X_train_scaled)\n",
    "    y_pred = gd_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    full_df = pd.concat([full_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    sg_full=LinearReg(eta=i, batch_size=10).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "    \n",
    "    y_train_pred = sg_full.predict(X_train_scaled)\n",
    "    y_pred = sg_full.predict(X_test_scaled)\n",
    "    \n",
    "    test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "    test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "    train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "    train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "    \n",
    "    temp_test_df = pd.DataFrame({'eta':[0.1*(10**(-i))],\n",
    "                            'test_mse': [test_mse],\n",
    "                       'test_r2': [-test_r2],\n",
    "                       'train_mse': [train_mse],\n",
    "                       'train_r2': [-train_r2]})\n",
    "    \n",
    "    sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 50)\n",
    "\n",
    "columns = ['batch_size', 'iter_count', 'test_mse', 'test_r2', 'train_mse', 'train_r2']\n",
    "sg_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(batch_size)\n",
    "    for i in range(5):\n",
    "\n",
    "        sg_full=LinearReg(eta=1e-3, batch_size=batch_size).fit(X_train_scaled, X_train['log_trip_duration'].to_numpy())\n",
    "\n",
    "        y_train_pred = sg_full.predict(X_train_scaled)\n",
    "        y_pred = sg_full.predict(X_test_scaled)\n",
    "\n",
    "        test_mse = mse(X_test['log_trip_duration'], y_pred)\n",
    "        test_r2 = r2_score(X_test['log_trip_duration'], y_pred)\n",
    "        train_mse = mse(X_train['log_trip_duration'], y_train_pred)\n",
    "        train_r2 = r2_score(X_train['log_trip_duration'], y_train_pred)\n",
    "\n",
    "        temp_test_df = pd.DataFrame({'batch_size':[batch_size],\n",
    "                                     'iter_count': [sg_full.real_iter],\n",
    "                                     'test_mse': [test_mse],\n",
    "                                     'test_r2': [-test_r2],\n",
    "                                     'train_mse': [train_mse],\n",
    "                                     'train_r2': [-train_r2]})\n",
    "\n",
    "        sg_df = pd.concat([sg_df, temp_test_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "sg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
