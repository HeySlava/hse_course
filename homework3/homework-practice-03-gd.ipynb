{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### О задании\n",
    "\n",
    "В данном задании необходимо реализовать обучение линейной регрессии с помощью различных вариантов градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним, что на лекциях и семинарах мы разбирали некоторые подходы к оптимизации функционалов по параметрам. В частности, был рассмотрен градиентный спуск и различные подходы к его реализации — стохастический, метод импульса и другие. В качестве модели у нас будет выступать линейная регрессия.\n",
    "\n",
    "В этом домашнем задании вам предстоит реализовать 4 различных вариации градиентного спуска, написать свою реализацию линейной регресии, сравнить методы градиентного спуска между собой на реальных данных и разобраться как подбирать гиперпараметры для этих методов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного спуска (3.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать собственные реализации различных подходов к градиентному спуску с опорой на подготовленные шаблоны в файле  `utils.py`:\n",
    "\n",
    "**Задание 1.1. (0.5 балла)** Полный градиентный спуск **GradientDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "**Задание 1.2. (1 балл)** Стохастический градиентный спуск **StochasticDescent**:\n",
    "\n",
    "$$\n",
    "    w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$ \n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\,$ - это оценка градиента по батчу объектов, выбранных случайно.\n",
    "\n",
    "**Задание 1.3. (1 балл)** Метод Momentum **MomentumDescent**:\n",
    "\n",
    "$$\n",
    "    h_0 = 0, \\\\\n",
    "    h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "    w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "**Задание 1.4. (1 балл)** Метод Adaptive gradient algorithm **Adagrad**:\n",
    "\n",
    "$$\n",
    "    G_0 = 0, \\\\\n",
    "    G_{k + 1} = G_{k} + \\left(\\nabla_{w} Q(w_{k})\\right) ^ 2, \\\\\n",
    "    w_{k + 1} = w_{k} - \\dfrac{\\eta_k}{\\sqrt{\\varepsilon + G_{k + 1}}} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "\n",
    "Во всех вышеназванных методах мы будем использовать следующую формулу для длины шага:\n",
    "\n",
    "$$\n",
    "    \\eta_{k} = \\lambda \\left(\\dfrac{s_0}{s_0 + k}\\right)^p\n",
    "$$\n",
    "На практике достаточно настроить параметр $\\lambda$, а остальным выставить параметры по умолчанию: $s_0 = 1, \\, p = 0.5.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать функцию потерь MSE:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "Все вычисления должны быть векторизованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализация линейной регресии (1.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит написать свою реализацию линейной регресии, обучаемой с использованием градиентного спуска, с опорой на подготовленные шаблоны в файле `utils.py` - **LinearRegression**.\n",
    "\n",
    "Необходимо соблюдать следующие условия:\n",
    "\n",
    "* Все вычисления должны быть векторизованы.\n",
    "* Циклы средствами python допускаются только для итераций градиентного спуска.\n",
    "* В качестве критерия останова необходимо использовать (одновременно):\n",
    "    * Квадрат евклидовой нормы разности весов на двух соседних итерациях меньше `tolerance`.\n",
    "    * Достижение максимального числа итераций `max_iter`.\n",
    "* Чтобы проследить за сходимостью оптимизационного процесса будем использовать `loss_history`, в нём будем хранить значения функции потерь до каждого шага, начиная с нулевого (до первого шага по антиградиенту).\n",
    "* Инициализировать веса нужно нулевым вектором или из нормального $\\mathcal{N}(0, 1)$ распределения (тогда нужно зафиксировать seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Проверка кода (0 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import (\n",
    "    Adagrad,\n",
    "    GradientDescent,\n",
    "    MomentumDescent,\n",
    "    StochasticDescent,\n",
    ")\n",
    "from utils import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 100\n",
    "dimension = 5\n",
    "\n",
    "X = np.random.rand(num_objects, dimension)\n",
    "y = np.random.rand(num_objects)\n",
    "\n",
    "lambda_ = 1e-2\n",
    "w0 = np.zeros(dimension)\n",
    "\n",
    "max_iter = 10\n",
    "tolerance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_default: float = 1\n",
    "p_default: float = 0.5\n",
    "\n",
    "batch_size_default: int = 1\n",
    "\n",
    "alpha_default: float = 0.1\n",
    "eps_default: float = 1e-8\n",
    "\n",
    "mu_default = 1e-2\n",
    "\n",
    "tolerance_default: float = 1e-3\n",
    "max_iter_default: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDescent:\n",
    "    \"\"\"\n",
    "    A base class and examples for all functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def step(self, X: np.ndarray, y: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Descent step\n",
    "        :param iteration: iteration number\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: difference between weights\n",
    "        \"\"\"\n",
    "        return self.update_weights(self.calc_gradient(X, y), iteration)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for update_weights function\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        if self.w == None:\n",
    "            self.w = np.zeros(np.shape(y))\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            self.w -= gradient\n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Example for calc_gradient function\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        print((X@self.w-y)@X /(len(y)))\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Full gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta * calc_gradient(X, y)\n",
    "            self.w -= step\n",
    "        return self\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientDescent\n",
    "\n",
    "descent = GradientDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient1 = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient1.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27609127, -0.25374032, -0.25545766, -0.24973055, -0.2815336 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, s0: float = s0_default, p: float = p_default,\n",
    "                 batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        :param batch_size: batch size (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_history = []\n",
    "        self.w = np.copy(w0)\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        for i in range(iteration):\n",
    "            step = self.eta*self.calc_gradient(X[self.batch_size], y[self.batch_size]) \n",
    "            self.w -= step\n",
    "            self.loss_history.append(mse(X@self.w, y))\n",
    "        return self\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StochasticDescent\n",
    "\n",
    "descent = StochasticDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumDescent(BaseDescent):\n",
    "    \"\"\"\n",
    "    Momentum gradient descent class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, alpha: float = alpha_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param alpha: momentum coefficient\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.alpha = alpha\n",
    "        self.w = np.copy(w0)\n",
    "        self.h = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        for i in range(iteration):\n",
    "            self.h = self.h*self.alpha + self.eta*self.calc_gradient(X, y)\n",
    "            self.w -= self.h\n",
    "        return self\n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MomentumDescent\n",
    "\n",
    "descent = MomentumDescent(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad(BaseDescent):\n",
    "    \"\"\"\n",
    "    Adaptive gradient algorithm class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0: np.ndarray, lambda_: float, eps: float = eps_default, s0: float = s0_default,\n",
    "                 p: float = p_default):\n",
    "        \"\"\"\n",
    "        :param w0: weight initialization\n",
    "        :param lambda_: learning rate parameter (float)\n",
    "        :param eps: smoothing term (float)\n",
    "        :param s0: learning rate parameter (float)\n",
    "        :param p: learning rate parameter (float)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eta = lambda k: lambda_ * (s0 / (s0 + k)) ** p\n",
    "        self.eps = eps\n",
    "        self.w = np.copy(w0)\n",
    "        self.g = 0\n",
    "\n",
    "    def update_weights(self, gradient: np.ndarray, iteration: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Changing weights with respect to gradient\n",
    "        :param iteration: iteration number\n",
    "        :param gradient: gradient estimate\n",
    "        :return: weight difference: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement updating weights function\n",
    "        self.g = np.zeros(np.shape(y))\n",
    "        for i in range(iteration):\n",
    "            k = np.random.randint(np.size(y))\n",
    "            self.g = self.g + calc_gradient(X[k], y[k]) ** 2\n",
    "            self.w = self.w - self.eta / ((self.g + self.eps)**0.5) * calc_gradient(X[k], y[k])\n",
    "        return self        \n",
    "\n",
    "    def calc_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getting objects, calculating gradient at point w\n",
    "        :param X: objects' features\n",
    "        :param y: objects' targets\n",
    "        :return: gradient: np.ndarray\n",
    "        \"\"\"\n",
    "        # TODO: implement calculating gradient function\n",
    "        return (X@self.w-y)@X /(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "\n",
    "descent = Adagrad(lambda_ = lambda_, w0 = w0)\n",
    "\n",
    "gradient = descent.calc_gradient(X, y)\n",
    "\n",
    "assert gradient.shape[0] == dimension, 'Gradient failed'\n",
    "\n",
    "# diff = descent.step(X, y, 0)\n",
    "\n",
    "# assert diff.shape[0] == dimension, 'Weights failed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(gradient1 == gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(BaseEstimator):\n",
    "    def __init__(self, gd_type='stochastic', \n",
    "                 tolerance=tolerance_default, max_iter=max_iter_default, w0=None, eta=1e-2,\n",
    "                batch_size: int = batch_size_default):\n",
    "        \"\"\"\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w0 \n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        if self.w==None:\n",
    "            self.w=np.zeros(np.size(X,1))\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "        if self.gd_type == 'full':\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                step=self.eta*self.calc_gradient(X,y) \n",
    "                w1=self.w.copy()\n",
    "                self.w -= step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance: \n",
    "                    print('break')\n",
    "                    break\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(self.max_iter):\n",
    "                step=self.eta*self.calc_gradient(X[self.batch_size],y[self.batch_size])  \n",
    "                w1=self.w.copy()\n",
    "                self.w = self.w-step\n",
    "                self.loss_history.append(self.calc_loss(X, y))\n",
    "                if np.linalg.norm(w1-self.w) < self.tolerance:\n",
    "                    print('break')\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = sm.add_constant(X)\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        return X.T.dot(X.dot(self.w)-y)/np.size(y)\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (ell, d)\n",
    "        y: np.array of shape (ell)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\"\n",
    "        return mse(X@self.w, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.arange(max_iter_default)), len(GD_full.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "GD_full=LinearReg(eta=0.01, batch_size=1).fit(X, y)\n",
    "GD_full\n",
    "\n",
    "sns.lineplot(x=np.arange(len(GD_full.loss_history)), y=GD_full.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'eta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b73d2487d475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss history failed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-36f2b92b3180>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'eta'"
     ]
    }
   ],
   "source": [
    "# LinearRegression\n",
    "\n",
    "regression = LinearRegression(\n",
    "    descent = StochasticDescent(lambda_ = lambda_, w0 = w0, batch_size = 2),\n",
    "    tolerance = tolerance,\n",
    "    max_iter = max_iter\n",
    ")\n",
    "\n",
    "regression.fit(X, y)\n",
    "\n",
    "assert len(regression.loss_history) == max_iter, 'Loss history failed'\n",
    "\n",
    "prediction = regression.predict(X)\n",
    "\n",
    "assert prediction.shape[0] == num_objects, 'Predict failed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваше решение прошло все тесты локально, то теперь осталось протестировать его в Яндекс Контесте - **https://contest.yandex.ru/contest/19551**.\n",
    "\n",
    "Для каждой задачи из контеста вставьте ссылку на успешную посылку:\n",
    "\n",
    "* **GradientDescent**: \n",
    "* **StochasticDescent**:\n",
    "* **MomentumDescent**:\n",
    "* **Adagrad**:\n",
    "* **LinearRegression**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Работа с данными (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем использовать датасет объявлений по продаже машин на немецком Ebay. В задаче предсказания целевой переменной для нас будет являться цена.\n",
    "Для дальнейшей работы сделайте следующее:\n",
    "* Проведите разумную предобработку данных.\n",
    "* Замените целевую переменную на её логарифм.\n",
    "* Разделите данные на обучающую, валидационную и тестовую выборки в отношении 3:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Сравнение методов градиентного спуска (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит сравнить методы градиентного спуска на подготовленных вами данных из предыдущего задания.\n",
    "\n",
    "* **Задание 5.1. (1.5 балла)** Подберите по валидационной выборке наилучшую длину $\\lambda$ шага для каждого метода. Для этого можно сделать перебор по логарифмической сетке, так как нас интересует скорее порядок величины, нежели её точное значение. Сравните качество методов по метрикам MSE и R^2 на обучающей и тестовой выборках, сравните количество итераций до сходимости. Все параметры кроме $\\lambda$ стоит выставить равным значениям по умолчанию.\n",
    "\n",
    "* **Задание 5.2. (0.5 балла)** Постройте график зависимости значения функции ошибки от номера итерации (все методы на одном графике).\n",
    "\n",
    "Посмотрите на получившиеся результаты. Сравните методы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.zeros(x_train.shape[1])\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Сходимость стохастического градиентного спуска в зависимости от размера батча (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние размера батча на работу стохастического градиентного спуска. \n",
    "\n",
    "* Сделайте по несколько запусков (например, k) стохастического градиентного спуска на обучающей выборке для каждого размера батча из списка. Замерьте время и количество итераций до сходимости. Посчитайте среднее и дисперсию этих значений для каждого размера батча.\n",
    "* Постройте график зависимости количества шагов до сходимости от размера батча.\n",
    "* Постройте график зависимости времени до сходимости от размера батча.\n",
    "\n",
    "Посмотрите на получившиеся результаты. Какие выводы можно сделать про подбор размера батча для стохастического градиентного спуска?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = np.arange(5, 500, 10)\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7. Регуляризация (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам предстоит исследовать влияние регуляризации на работу различных методов градиентного спуска. (Напомним, регуляризация - это добавка к функции потерь, которая штрафует за норму весов). Мы будем использовать l2 регуляризацию, таким образом функция потерь приобретает следующий вид:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\dfrac{1}{\\ell} \\sum\\limits_{i=1}^{\\ell} (a_w(x_i) - y_i)^2 + \\dfrac{\\mu}{2} \\| w \\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допишите классы **GradientDescentReg**, **StochasticDescentReg**, **MomentumDescentReg**, **AdagradReg** в файле `utils.py`. Мы будем использовать тот же самый класс для линейной регрессии, так как для сравнения методов с регуляризацией и без неё нам нужна только MSE часть функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите по валидационной выборке лучшие параметры обучения с регуляризацией. Сравните для каждого метода результаты на тестовой выборке по метрикам MSE и R^2 с регуляризацией и без регуляризации. Постройте для каждого метода график со значениями функции потерь MSE с регуляризацией и без регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите на получившиеся результаты. Какие можно сделать выводы, как регуляризация влияет на сходимость? Чем вы можете объяснить это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from utils import (\n",
    "    AdagradReg,\n",
    "    GradientDescentReg,\n",
    "    MomentumDescentReg,\n",
    "    StochasticDescentReg,\n",
    ")\n",
    "\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8. Бонус — Реализация метода SAG (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве бонуса вам будет следующее задание - напишите собственную реализацию стохастического градиентного спуска по методу SAG в файле `utils.py`. Подробнее прочитать про SAG можно [здесь](https://arxiv.org/pdf/1309.2388.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните свою реализацию метода SAG с обычным полным градиентным спуском на наших данных. Проведите сравнение аналогично заданию 5. Что вы можете сказать про сходимость этого метода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вставьте картинку или видео, описывающие ваш опыт выполнения этого задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
